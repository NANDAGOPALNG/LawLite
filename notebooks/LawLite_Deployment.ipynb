{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LawLite: Fine-tuned LLaMA 2 Deployment with Gradio\n",
        "\n",
        "This notebook demonstrates how to deploy a fine-tuned LLaMA 2 model using Gradio.  \n",
        "It includes steps for loading the model, setting up the interface, and running it locally or on Hugging Face Spaces.\n"
      ],
      "metadata": {
        "id": "rcHZ4879kv_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Installing Required Packages\n",
        "We begin by installing the necessary libraries for this project:  \n",
        "- **transformers** ‚Üí for loading and using the LLaMA-2 model.  \n",
        "- **peft** ‚Üí for parameter-efficient fine-tuning.  \n",
        "- **bitsandbytes** ‚Üí for 8-bit/4-bit quantization to save memory.  \n",
        "- **accelerate** ‚Üí to optimize model training/inference across devices.  \n",
        "- **gradio** ‚Üí to build a simple interactive web UI.  \n",
        "- **torch** ‚Üí PyTorch, the deep learning framework powering our model."
      ],
      "metadata": {
        "id": "TyISzLrzYsEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sKQBPv0-Vdt"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install -q transformers\n",
        "!pip install -q peft\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q accelerate\n",
        "!pip install -q gradio\n",
        "!pip install -q torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Importing Libraries\n",
        "Here, we import all the Python libraries needed for deployment.  \n",
        "This includes PyTorch, Gradio for the UI, Transformers for model handling,  \n",
        "and additional utilities for model loading, tokenization, and warnings.\n"
      ],
      "metadata": {
        "id": "kGlNHRTHYxzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import necessary libraries\n",
        "import torch\n",
        "import gradio as gr\n",
        "import zipfile\n",
        "import os\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    LlamaTokenizer\n",
        ")\n",
        "from peft import PeftModel\n",
        "from google.colab import files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "csTUuMYnF4ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Checking GPU Availability\n",
        "Since large models like LLaMA-2 require significant compute,  \n",
        "we check whether a GPU is available in this environment.  \n",
        "If available, it prints the GPU name and memory size; otherwise, it falls back to CPU.\n"
      ],
      "metadata": {
        "id": "IxL7n_efc0W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Check GPU availability and setup\n",
        "print(\"Checking GPU availability...\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU available - using CPU (will be slower)\")\n"
      ],
      "metadata": {
        "id": "6aFZ7hpQmK-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîë Logging into Hugging Face\n",
        "We log in to Hugging Face Hub to authenticate.  \n",
        "This is required if you‚Äôre pulling private models or pushing your model/app to the Hub.\n"
      ],
      "metadata": {
        "id": "dAf52aS-c3Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "JLvcsjGZnihJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Uploading the Fine-tuned Model\n",
        "Here, we upload our **fine-tuned model zip file** into the notebook.  \n",
        "The uploaded file will later be extracted and used for inference.\n"
      ],
      "metadata": {
        "id": "xnnc_aNqbGuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Upload and extract your fine-tuned model\n",
        "print(\"\\nüìÅ Please upload your fine-tuned model zip file...\")\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Hv2r1qsfjqMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the uploaded file name\n",
        "zip_filename = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded: {zip_filename}\")"
      ],
      "metadata": {
        "id": "VpGEu7oCmX4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Extracting the Uploaded Model\n",
        "Once the model zip file is uploaded, we extract it into the working directory.  \n",
        "This makes the model weights, tokenizer files, and configuration accessible for loading.\n"
      ],
      "metadata": {
        "id": "k79m0LtUdDhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the model\n",
        "print(\"Extracting model files...\")\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n"
      ],
      "metadata": {
        "id": "i7Xa1fHMme05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List extracted contents to find model path\n",
        "print(\"Extracted files:\")\n",
        "for root, dirs, files in os.walk(\"/content/\"):\n",
        "    for file in files:\n",
        "        if file.endswith('.bin') or file.endswith('.safetensors') or file == 'adapter_config.json':\n",
        "            print(os.path.join(root, file))"
      ],
      "metadata": {
        "id": "PFcJ62-HmjUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to adjust this path based on your zip structure\n",
        "MODEL_PATH = \"/content/finetunedModel/checkpoint-20\""
      ],
      "metadata": {
        "id": "Tk5zzW1gnGGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Configure quantization for memory efficiency\n",
        "print(\"\\nüîß Setting up model configuration...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "rTXUKSbPKycl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Loading the Base Model and Tokenizer\n",
        "We now load the **base LLaMA model** and its tokenizer from Hugging Face.  \n",
        "The model is set up with quantization support (via bitsandbytes) to reduce memory usage.\n"
      ],
      "metadata": {
        "id": "hb2V7itWdKs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load base model and tokenizer\n",
        "print(\"üì¶ Loading base LLaMA-2-7B model...\")\n",
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "id": "IOOIUkWYnNsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîó Attaching Fine-tuned Weights\n",
        "Using **PEFT (Parameter Efficient Fine-Tuning)**,  \n",
        "we merge our fine-tuned adapter weights into the base model.  \n",
        "This allows the model to retain LLaMA‚Äôs knowledge while applying domain-specific fine-tuning for legal text simplification."
      ],
      "metadata": {
        "id": "TeFAm3m1d08U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    use_fast=False,\n",
        "    trust_remote_code=True,\n",
        "    add_eos_token=True\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "\n",
        "# Load base model with quantization\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "IcHUtUv7n1an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Preparing the Model for Inference\n",
        "We move the model to the appropriate device (GPU/CPU)  \n",
        "and set it to **evaluation mode** to ensure efficient inference without unnecessary training overhead.\n"
      ],
      "metadata": {
        "id": "HIhoEAswdRmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Load your fine-tuned LoRA adapters\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig # Import PeftConfig\n",
        "\n",
        "print(\"\\nüì¶ Loading fine-tuned LoRA adapters...\")\n",
        "\n",
        "# Load the base model first (already done in cell IcHUtUv7n1an)\n",
        "# Make sure 'base_model' is available in the environment\n",
        "\n",
        "try:\n",
        "    # Load the PEFT configuration from the local path\n",
        "    peft_config = PeftConfig.from_pretrained(MODEL_PATH)\n",
        "\n",
        "    # Load the adapter weights onto the base model using PeftModel\n",
        "    # The base_model should already be quantized as per previous steps\n",
        "    model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
        "\n",
        "    print(f\"‚úÖ Successfully loaded adapter from {MODEL_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to load adapter from {MODEL_PATH}. Error: {e}\")\n",
        "    print(\"Please ensure the zip file extracted the adapter files (like adapter_config.json and adapter_model.bin/safetensors) correctly to the specified MODEL_PATH.\")\n",
        "    print(\"You can check the contents of the directory using: !ls -lha {MODEL_PATH}\")\n",
        "    # Add a check to see if the adapter_config.json exists\n",
        "    import os\n",
        "    config_path = os.path.join(MODEL_PATH, 'adapter_config.json')\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"Error: adapter_config.json not found at {config_path}\")\n",
        "\n",
        "# The 'model' variable now holds the base model with the loaded adapter\n",
        "# Ensure the model is in evaluation mode for inference\n",
        "if 'model' in locals() and model is not None:\n",
        "    model.eval()\n",
        "    print(\"Model is ready for inference.\")\n",
        "else:\n",
        "    print(\"Model could not be loaded. Please check the model path and extracted files.\")"
      ],
      "metadata": {
        "id": "59OTKEh6n5VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Defining the Response Function\n",
        "Here we define a function that:\n",
        "1. Accepts user input (legal text).  \n",
        "2. Tokenizes the input.  \n",
        "3. Runs the model to generate an output.  \n",
        "4. Decodes the tokens back into natural language.  \n",
        "\n",
        "This function will later be connected to our Gradio interface.\n"
      ],
      "metadata": {
        "id": "VLKatorKdV3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Define inference function\n",
        "def generate_legal_response(user_question, max_tokens=512, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generate response from the fine-tuned legal model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Format the prompt\n",
        "        prompt = f\"Question: {user_question}\\nAnswer this legal question accurately and concisely:\\n\"\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Generate response\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.1,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode and clean response\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the generated part (after the prompt)\n",
        "        if \"Answer this legal question accurately and concisely:\" in full_response:\n",
        "            response = full_response.split(\"Answer this legal question accurately and concisely:\")[-1].strip()\n",
        "        else:\n",
        "            response = full_response.replace(prompt, \"\").strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\""
      ],
      "metadata": {
        "id": "Mj8MaNubpsE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé® Creating the Gradio Interface\n",
        "We now build a simple **Gradio interface** with:  \n",
        "- A text box for user input.  \n",
        "- A text output area for the simplified legal explanation.  \n",
        "This provides an interactive way for anyone to use the model in their browser.\n"
      ],
      "metadata": {
        "id": "SrHMYbemdXuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Create Gradio interface\n",
        "def chat_interface(question, max_tokens, temperature):\n",
        "    \"\"\"Gradio interface function\"\"\"\n",
        "    if not question.strip():\n",
        "        return \"Please enter a legal question.\"\n",
        "\n",
        "    response = generate_legal_response(question, max_tokens, temperature)\n",
        "    return response\n",
        "\n",
        "# Step 10: Launch Gradio app\n",
        "print(\"\\nüöÄ Launching LawLite Legal AI Interface...\")\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=chat_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            lines=4,\n",
        "            placeholder=\"Enter your legal question here...\",\n",
        "            label=\"Legal Question\"\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=50,\n",
        "            maximum=1024,\n",
        "            value=512,\n",
        "            step=50,\n",
        "            label=\"Max Response Length\"\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=1.0,\n",
        "            value=0.7,\n",
        "            step=0.1,\n",
        "            label=\"Temperature (Creativity)\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"LawLite Response\", lines=8),\n",
        "    title=\"‚öñÔ∏è LawLite: AI Legal Assistant\",\n",
        "    description=\"\"\"\n",
        "    **LawLite** is a fine-tuned LLaMA-2-7B model specialized for Indian legal questions.\n",
        "    Ask questions about legal documents, procedures, or seek clarification on legal matters.\n",
        "\n",
        "    *Note: This is an AI assistant and responses should not be considered as professional legal advice.*\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        [\"What is the procedure for filing an appeal in the High Court?\", 512, 0.7],\n",
        "        [\"Explain the concept of bail under Indian law\", 400, 0.6],\n",
        "        [\"What are the key provisions of Section 66 of the Indian Income Tax Act?\", 600, 0.7]\n",
        "    ],\n",
        "    theme=gr.themes.Soft(),\n",
        "    allow_flagging=\"never\"\n",
        ")"
      ],
      "metadata": {
        "id": "7AYcTPsYwAIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Launching the Gradio App\n",
        "Finally, we launch the Gradio app.  \n",
        "Once running, you‚Äôll get a public URL where anyone can interact with your fine-tuned model.\n"
      ],
      "metadata": {
        "id": "JoKF2SxddbWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        debug=True,\n",
        "        share=True,  # Creates public link\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        show_error=True\n",
        "    )"
      ],
      "metadata": {
        "id": "KiugA17mwzMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G3-rcPBbw3_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}