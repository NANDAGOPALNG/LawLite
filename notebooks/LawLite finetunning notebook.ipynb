{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**\n",
        "# LawLite ⚖️\n",
        "\n",
        "**LawLite** is a fine-tuned Large Language Model (LLM) designed to simplify complex legal documents into clear, plain-English summaries. Built on top of open-source LLaMA-2-7B model and optimized using **LoRA/QLoRA** techniques, LawLite helps users quickly understand lengthy contracts, agreements, and compliance documents without legal jargon.\n",
        "\n",
        "With a focus on **accuracy, clarity, and accessibility**, LawLite bridges the gap between legal professionals and everyday readers by turning pages of dense text into concise, actionable insights.\n",
        "\n",
        "here i am going to use this dataset to fine tune the model: https://zenodo.org/records/7152317#.Yz6mJ9JByC0"
      ],
      "metadata": {
        "id": "j7knRSOZn1Pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell installs the required Python libraries for the project:\n",
        "\n",
        "* peft → for parameter-efficient fine-tuning of large models\n",
        "\n",
        "* accelerate → to easily manage multi-GPU / distributed training\n",
        "\n",
        "* bitsandbytes → for memory-efficient 8-bit/4-bit model training\n",
        "\n",
        "* transformers → Hugging Face’s library for working with LLMs\n",
        "\n",
        "* datasets → for handling and processing datasets"
      ],
      "metadata": {
        "id": "4_wK4r10n7QN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gborTXJww0js"
      },
      "outputs": [],
      "source": [
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandBytes\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs GPUtil, a utility library that helps in checking the current GPU usage (memory, load, etc.), which is useful before training large models."
      ],
      "metadata": {
        "id": "E18xl3rpoQLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil"
      ],
      "metadata": {
        "id": "7JojUM3f6Ol-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here in this cell we are going to:\n",
        "\n",
        "* Imports PyTorch, GPUtil, and os.\n",
        "\n",
        "* Displays current GPU utilization with GPUtil.showUtilization().\n",
        "\n",
        "* Checks if a GPU is available for training and prints the status.\n",
        "\n",
        "* Configures CUDA environment variables to ensure GPU device ordering and explicitly sets the visible device to GPU 0."
      ],
      "metadata": {
        "id": "OrQFsm-ooy6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import GPUtil\n",
        "import os\n",
        "\n",
        "GPUtil.showUtilization()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available, using CPU instead\")\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "kK4ucngS6opL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports all the necessary Hugging Face and PEFT utilities.\n",
        "\n",
        "* AutoTokenizer / AutoModelForCausalLM → load pre-trained LLMs\n",
        "\n",
        "* BitsAndBytesConfig → configure quantization\n",
        "\n",
        "* LlamaTokenizer → specific tokenizer for LLaMA models\n",
        "\n",
        "* notebook_login → authenticate with Hugging Face Hub\n",
        "\n",
        "* peft utilities for LoRA fine-tuning"
      ],
      "metadata": {
        "id": "E8ZXt9R7pYos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "qBYpRDOu6tPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticates with Hugging Face Hub.\n",
        "\n",
        "Uses CLI login if on Google Colab.\n",
        "\n",
        "Otherwise, uses the notebook_login() widget."
      ],
      "metadata": {
        "id": "nrxWWKmgplwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"COLAB_GPU\" in os.environ:\n",
        "  !huggingface-cli login\n",
        "else:\n",
        "  notebook_login()"
      ],
      "metadata": {
        "id": "I_wC8uw_7zUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell loads the LLaMA 2–7B chat model in 4-bit quantized format using bitsandbytes.\n",
        "This drastically reduces memory usage, making training possible on limited GPUs."
      ],
      "metadata": {
        "id": "7MC4vnEWpuFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
      ],
      "metadata": {
        "id": "csddUu0Y-SLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have already uploaded a dataset in zipflie format which consists of the supreme court judgements of Indian supreme court and UK supreme court which is already divided into train and test data\n",
        "\n",
        "you can also download the data from here:https://zenodo.org/records/7152317#.Yz6mJ9JByC0\n",
        "\n",
        "the below cell handles dataset upload and extraction:\n",
        "\n",
        "* Assumes a .zip dataset is uploaded.\n",
        "\n",
        "* Extracts it to /content/dataset/.\n",
        "\n",
        "* Lists files to confirm extraction."
      ],
      "metadata": {
        "id": "yGSkgGzLp5MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to your uploaded zip\n",
        "zip_path = \"/content/dataset.zip\"\n",
        "\n",
        "# Unzip into /content/dataset/\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset\")\n",
        "\n",
        "# Check extracted files\n",
        "os.listdir(\"/content/dataset\")"
      ],
      "metadata": {
        "id": "2KPIk3Ws-fxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads Indian legal judgment dataset into Hugging Face datasets.\n",
        "\n",
        "* Reads .txt files from train-data and test-data folders.\n",
        "\n",
        "* Creates train and test datasets.\n",
        "\n",
        "* Prints dataset details."
      ],
      "metadata": {
        "id": "a2x0Cr3hq0Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Paths to train/test\n",
        "base_path = \"/content/dataset/dataset/IN-Abs\"\n",
        "train_path = os.path.join(base_path, \"train-data/judgement\")\n",
        "test_path  = os.path.join(base_path, \"test-data/judgement\")\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files={\"train\": [os.path.join(train_path, f) for f in os.listdir(train_path) if f.endswith(\".txt\")]},\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "test_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files={\"test\": [os.path.join(test_path, f) for f in os.listdir(test_path) if f.endswith(\".txt\")]},\n",
        "    split=\"test\"\n",
        ")\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "UFRRkLeOAtoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell performs foll action:\n",
        "\n",
        "* Loads tokenizer for the LLaMA 2 model.\n",
        "\n",
        "* Ensures there is a padding token (uses EOS token if missing)."
      ],
      "metadata": {
        "id": "10bV04CWq68J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
      ],
      "metadata": {
        "id": "enLdDtD2BcFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell tokenizes the training dataset:\n",
        "\n",
        "* Converts each text sample into tokens.\n",
        "\n",
        "* Applies truncation and padding up to 512 tokens."
      ],
      "metadata": {
        "id": "hXbsMnfHrKMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the train dataset from your uploaded IN data\n",
        "tokenized_train_dataset = []\n",
        "for phrase in train_dataset:\n",
        "    tokenized = tokenizer(phrase[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    tokenized_train_dataset.append(tokenized)\n"
      ],
      "metadata": {
        "id": "c0tgGUwJDP00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displays one tokenized example from the training dataset."
      ],
      "metadata": {
        "id": "O77ucYJ9rdMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset[1]"
      ],
      "metadata": {
        "id": "fgonIpnpGG21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks what the End-of-Sequence (EOS) token is for the tokenizer."
      ],
      "metadata": {
        "id": "yiuGHkPErebk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "id": "dBXmFFoSIFKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepares the model for LoRA fine-tuning:\n",
        "\n",
        "* Enables gradient checkpointing (saves memory).\n",
        "\n",
        "* Defines a LoRA config with rank=8, dropout=0.05.\n",
        "\n",
        "* Applies LoRA adapters to attention and projection layers."
      ],
      "metadata": {
        "id": "L7LBqsJerq_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "ykz21OGvIL3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell sets up Hugging Face Trainer for fine-tuning:\n",
        "\n",
        "* Uses LoRA-modified LLaMA model.\n",
        "\n",
        "* Batch size = 2, with gradient accumulation = 2.\n",
        "\n",
        "* Trains for 3 epochs (or max 20 steps).\n",
        "\n",
        "* Uses paged AdamW optimizer with 8-bit efficiency.\n",
        "\n",
        "* Saves checkpoints and logs.\n",
        "\n",
        "* Starts training with trainer.train()."
      ],
      "metadata": {
        "id": "66BHs9zisIxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=\"./finetunedModel\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=1e-4,\n",
        "        max_steps=20,\n",
        "        bf16=False,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_dir=\"./log\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_steps=50,\n",
        "        logging_steps=10\n",
        "\n",
        "),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache=False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3kKrCDgKIaXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reloads the base LLaMA 2 model with 4-bit quantization for inference."
      ],
      "metadata": {
        "id": "V03vadvWsXQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, LlamaTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "nf4Config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=nf4Config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        "  )"
      ],
      "metadata": {
        "id": "mvdSHJ2vIfYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads the fine-tuned LoRA model checkpoint on top of the base LLaMA 2 model."
      ],
      "metadata": {
        "id": "y94vzSCasoyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "modelFinetuned = PeftModel.from_pretrained(base_model, \"finetunedModel/checkpoint-20\")"
      ],
      "metadata": {
        "id": "GwO2MctqMTF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell tests the fine-tuned model:\n",
        "\n",
        "* Defines a legal question prompt.\n",
        "\n",
        "* Tokenizes it and sends it to the model.\n",
        "\n",
        "* Generates a response (up to 1024 tokens).\n",
        "\n",
        "* Prints the model’s prediction."
      ],
      "metadata": {
        "id": "zFLnAWL4st6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Appeal from the High Court of judicature, Bombay, in a reference under section 66 of the Indian Income tax Act, 1022.K.M. Munshi , for the lant.  M.C. Setalvad, Attorney General for India, for the respondent. 1950.\"\n",
        "\n",
        "eval_prompt = f\"Question: {user_question} Just answer this question accurately and concisely.\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens=1024)[0], skip_special_tokens=True))\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "uqWYyihtNFSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create a Gradio interface to see the working"
      ],
      "metadata": {
        "id": "HNYPzY-F0UOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/install Gradio\n",
        "try:\n",
        "    import gradio as gr\n",
        "except:\n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "id": "v2n83PuzBUx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Inference function\n",
        "def chat_with_model(user_input):\n",
        "    prompt = f\"Question: {user_input}\\nAnswer concisely:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Gradio UI\n",
        "demo = gr.Interface(\n",
        "    fn=chat_with_model,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter a legal question...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"⚖️ LawLite: Legal AI Assistant\",\n",
        "    description=\"Ask me Indian legal questions, and I will provide concise answers using a fine-tuned LLaMA model.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True,share=True)"
      ],
      "metadata": {
        "id": "YcK74hOZNaws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zmqZzlI80fTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}